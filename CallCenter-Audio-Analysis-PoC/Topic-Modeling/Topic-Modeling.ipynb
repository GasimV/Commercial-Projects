{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835d2a82-04c4-4ef3-9f7b-ef905521daca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚úÖ Summary of the Approach\n",
    "\n",
    "1. **Input**: Bank call center **audio recordings**\n",
    "2. **Step 1 ‚Äì Transcription**: Use **Whisper Large** to transcribe calls ‚Üí Get **text data**\n",
    "3. **Step 2 ‚Äì Labeling**: Use ***Clustering*** and ***ChatGPT or another LLM*** to infer **topics/labels** from transcriptions\n",
    "4. **Step 3 ‚Äì Dataset Creation**: Use transcribed and labeled examples as your **training data**\n",
    "5. **Step 4 ‚Äì Vectorization**: Use **TF-IDF** or **BERT to vectorize via embeddings** the transcriptions\n",
    "6. **Step 5 ‚Äì Classification**: Train a **classical ML model** (Logistic Regression, SVM, etc.) to classify future calls by topic\n",
    "7. **Step 6 ‚Äì Inference**: For new calls ‚Üí vectorize ‚Üí classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ec350-17a7-49f0-991d-3db65ac4e4fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transcribing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39abd2-983d-4ce5-b3c0-6a51e6f1d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from stable_whisper import load_model\n",
    "\n",
    "# === SETTINGS ===\n",
    "root_dir = r\"C:\\Pasha-PoC\\Audio-Data\"\n",
    "output_csv = r\"C:\\Pasha-PoC\\Topic-Modeling\\transcriptions.csv\"  # ‚úÖ Save here\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model = load_model(\"large-v3\", device=device)\n",
    "\n",
    "# === HELPER FUNCTION ===\n",
    "def transcribe_audio_files(directory):\n",
    "    results = []\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith((\".wav\", \".mp3\", \".m4a\")):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                print(f\"üéß Transcribing: {filepath}\")\n",
    "\n",
    "                try:\n",
    "                    result = model.transcribe(filepath, language=\"az\")\n",
    "                    text = result.text.strip()  # ‚úÖ Access object attribute, not dict\n",
    "\n",
    "                    if text:  # Skip empty transcriptions\n",
    "                        results.append([text, 1])  # Dummy label '1'\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to transcribe {filename}: {e}\")\n",
    "    return results\n",
    "\n",
    "# === PROCESS ===\n",
    "transcriptions = transcribe_audio_files(root_dir)\n",
    "\n",
    "# === SAVE TO CSV ===\n",
    "if transcriptions:\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)  # ‚úÖ Ensure output dir exists\n",
    "\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Transcription\", \"Label\"])  # Header\n",
    "        writer.writerows(transcriptions)\n",
    "\n",
    "    print(f\"‚úÖ Transcriptions saved to {output_csv}\")\n",
    "else:\n",
    "    print(\"‚ùå No transcriptions were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f24384-7973-4346-9764-0508313edeeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6f96a-c16d-4e49-9cf8-d88c31c9ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Pasha-PoC\\Topic-Modeling\\transcriptions.csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df['Transcription'].iloc[0])\n",
    "print()\n",
    "display(df.index)\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe12e2-0f65-4dda-9ecd-fa913ebcde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e458b-1163-4f89-9492-dbd2866e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transcription'].loc[df['Transcription'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb64de-30af-49bb-bb45-07782c7fc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for very short or likely meaningless transcriptions (e.g., less than 5 words or characters)\n",
    "short_transcriptions = df[df[\"Transcription\"].str.split().str.len() < 24]\n",
    "\n",
    "# Also check for transcriptions with only repeating characters or gibberish\n",
    "suspicious_transcriptions = df[df[\"Transcription\"].str.fullmatch(r'[\\w\\s\\.\\,\\-\\']{0,10}')]\n",
    "\n",
    "# Combine the two filters\n",
    "potentially_meaningless = pd.concat([short_transcriptions, suspicious_transcriptions]).drop_duplicates()\n",
    "\n",
    "potentially_meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3f03a-3fff-4b21-9f06-1d247171bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Transcription'].iloc[41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19bb2aa-e59f-45ed-b452-f12057eba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the potentially meaningless transcriptions\n",
    "cleaned_df = df.drop(index=potentially_meaningless.index).reset_index(drop=True)\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6fd49-99e3-4189-a20c-023da3d94205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file_path = \"C:/Pasha-PoC/Topic-Modeling/transcriptions.csv\"\n",
    "cleaned_df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f5436-40de-4c17-9b8b-b4da05ac6893",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890fb9f-b997-43fa-bf6f-f3cadb1ac8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Pasha-PoC\\Topic-Modeling\\Labeled_Transcriptions.csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df['Transcription'].iloc[0])\n",
    "print()\n",
    "display(df.index)\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be5739-b2f2-4d77-b346-fe40653fcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e7040-8736-4d87-8c4a-285ef38700f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(df['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bea6f-0bfe-4243-8f7d-83ec1b7fd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].replace({\n",
    "    'General Inquiry': 'Other',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7de4b-ad23-4421-a934-d4bdf94fb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(df['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455f680-4f3a-4ba9-9b5b-fe1f3cc090c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=9\n",
    "display(df['Transcription'].iloc[n])\n",
    "print()\n",
    "display(df['Label'].iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2e471-21ea-4931-8c71-77029d355884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update label for the transcription at index N\n",
    "N = 1\n",
    "df.at[N, 'Label'] = \"Account Statement\"\n",
    "display(df.iloc[N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144b970-62db-4a14-84ca-2c8a8cd42914",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file_path = \"C:/Pasha-PoC/Topic-Modeling/Labeled_Transcriptions.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995cda0c-078d-45af-b6b2-fbf613ddb27a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3cc4-92a7-45f6-b98b-d3dda74e9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Pasha-PoC\\Topic-Modeling\\Labeled_Transcriptions.csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d13054-96f3-4f07-b76a-5b223f892969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-uncased').to(device)\n",
    "model.eval()\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for text in df['Transcription']:\n",
    "    # Tokenize and move to device\n",
    "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        cls_embedding = output.pooler_output.detach().cpu().numpy().squeeze()  # move back to CPU for numpy\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "X = np.vstack(embeddings)  # Feature matrix\n",
    "y = df['Label'].values      # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4487d-eed1-4de2-945a-7d03a3bd4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Combine embeddings and labels into a DataFrame\n",
    "df_embeddings = pd.DataFrame(X)\n",
    "df_embeddings[\"Label\"] = y  # Append label column\n",
    "\n",
    "# Define output path\n",
    "output_path = r\"C:/Pasha-PoC/Topic-Modeling/train_data.csv\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_embeddings.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Embeddings saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a55d0-6e17-4d88-9b54-f35c519a89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:/Pasha-PoC/Topic-Modeling/train_data.csv\")\n",
    "display(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32488a68-8d23-4e9b-a215-e17a02dfeb52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a02e9-ce86-4f18-a762-25ba4f16bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(r\"C:/Pasha-PoC/Topic-Modeling/train_data.csv\")\n",
    "\n",
    "# Features and encoded labels\n",
    "X = df.drop(columns=[\"Label\"]).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Label\"])  # ‚úÖ Correct way to encode labels\n",
    "class_names = le.classes_          # Optional: useful for decoding\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1_000, random_state=42))\n",
    "    ]),\n",
    "    \"SVM (RBF)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(random_state=42))\n",
    "    ]),\n",
    "    \"K-NN\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Evaluating: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"‚úÖ Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08348926-acf3-4929-9c29-8119a8b69599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(models[\"Logistic Regression\"], \"C:/Pasha-PoC/Topic-Modeling/lrm.pkl\")\n",
    "\n",
    "# Save LabelEncoder\n",
    "joblib.dump(le, \"C:/Pasha-PoC/Topic-Modeling/label_encoder.pkl\")\n",
    "\n",
    "print(\"‚úÖ Model and LabelEncoder saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da246c8-96a9-4b1a-8e6d-93fd87bef7c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a0a0c-cfe8-4054-9af4-c43278974576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Pasha-PoC/Topic-Modeling/train_data.csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df.drop(columns=[\"Label\"]).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Label\"])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Neural network model\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y))\n",
    "epochs = 300\n",
    "lr = 0.01\n",
    "batch_size = 27\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = FeedforwardNN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train_tensor.size()[0])\n",
    "    for i in range(0, X_train_tensor.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation (test) set every epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred_probs = model(X_test_tensor)\n",
    "        y_val_pred = torch.argmax(y_val_pred_probs, axis=1).numpy()\n",
    "        val_acc = accuracy_score(y_test, y_val_pred)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_test_tensor)\n",
    "    y_pred = torch.argmax(y_pred_probs, axis=1).numpy()\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n‚úÖ Test Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=np.unique(df[\"Label\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67e4ba-8aa2-4e3a-861c-3fdaf8758014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "torch.save(model.state_dict(), \"C:/Pasha-PoC/Topic-Modeling/ffnn_model.pth\")\n",
    "joblib.dump(scaler, \"C:/Pasha-PoC/Topic-Modeling/scaler.pkl\")\n",
    "joblib.dump(le, \"C:/Pasha-PoC/Topic-Modeling/label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26364df-fe65-4677-b2a2-6fddfe73d38e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20795464-1641-4956-aed7-3a551c5f3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Pasha-PoC\\Topic-Modeling\\Labeled_Transcriptions.csv\")\n",
    "display(df)\n",
    "print()\n",
    "display(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4f4aa-f42c-4e22-9e0a-099c38775009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transcription'].iloc[84]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9c18-e7aa-4c65-bf48-87a9ad678af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debd541-a9ee-4a57-b39e-7ecc7dd873a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Pasha-PoC\\Topic-Modeling\\topics.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697d5e8-4977-4fdc-b45e-3f0297fc932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import gradio as gr\n",
    "import webbrowser\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "from stable_whisper import load_model as load_sw_model\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# ========== Setup ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEBUG_DIR = \"debug/\"\n",
    "RECORDER_DIR = \"records\"\n",
    "PROCESSED_DIR = os.path.join(RECORDER_DIR, \"processed\")\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "os.makedirs(RECORDER_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ========== Models ==========\n",
    "# Load BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\").to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# Load scaler and label encoder\n",
    "scaler = joblib.load(\"C:/Pasha-PoC/Topic-Modeling/scaler.pkl\")\n",
    "label_encoder = joblib.load(\"C:/Pasha-PoC/Topic-Modeling/label_encoder.pkl\")\n",
    "\n",
    "# Load FFNN model\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = 768\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = FeedforwardNN(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load(\"C:/Pasha-PoC/Topic-Modeling/ffnn_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Load Whisper\n",
    "sw_model = load_sw_model(\"large-v3\", device=device)\n",
    "\n",
    "# Load Denoiser\n",
    "denoise_model = pretrained.dns64().to(device)\n",
    "\n",
    "def denoise_audio(audio_path):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    wav = convert_audio(wav, sr, denoise_model.sample_rate, denoise_model.chin)\n",
    "    with torch.no_grad():\n",
    "        enhanced = denoise_model(wav.to(device))\n",
    "    enhanced = enhanced.squeeze(0).cpu()\n",
    "    out_path = os.path.join(DEBUG_DIR, f\"denoised_{uuid.uuid4().hex}.wav\")\n",
    "    torchaudio.save(out_path, enhanced, denoise_model.sample_rate)\n",
    "    return out_path\n",
    "\n",
    "# ========== Classification ==========\n",
    "def classify_with_ffnn(text):\n",
    "    text = text.lower().strip()\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        output = bert_model(**encoded)\n",
    "        cls_embedding = output.pooler_output.squeeze().cpu().numpy()\n",
    "        cls_scaled = scaler.transform([cls_embedding])\n",
    "        cls_tensor = torch.tensor(cls_scaled, dtype=torch.float32)\n",
    "        logits = model(cls_tensor)\n",
    "        predicted_class = torch.argmax(logits, axis=1).item()\n",
    "        label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "        return label\n",
    "\n",
    "# ========== Full Pipeline ==========\n",
    "def process_audio_and_classify(audio_path):\n",
    "    raw_copy = os.path.join(DEBUG_DIR, f\"original_{uuid.uuid4().hex}.wav\")\n",
    "    shutil.copy(audio_path, raw_copy)\n",
    "\n",
    "    denoised_path = denoise_audio(audio_path)\n",
    "\n",
    "    result = sw_model.transcribe(denoised_path, language=\"azerbaijani\", word_timestamps=False)\n",
    "    full_text = result.text.strip()\n",
    "\n",
    "    label = classify_with_ffnn(full_text)\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <h3>üîä Denoised Audio</h3>\n",
    "    <audio controls src='{denoised_path}' style='width:100%; margin-bottom:16px;'></audio>\n",
    "    <h3>üìÑ Transcription</h3>\n",
    "    <div style='white-space: pre-wrap; border:1px solid #ccc; padding:8px;'>{full_text}</div>\n",
    "    <h3>ü§ñ Topic Prediction</h3>\n",
    "    <div style='font-size: 1.2em; font-weight: bold;'>{label}</div>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "# ========== Watchdog ==========\n",
    "class NewWavHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        if event.is_directory or not event.src_path.endswith(\".wav\"):\n",
    "            return\n",
    "        if \"processed\" in os.path.normpath(event.src_path).split(os.sep):\n",
    "            return\n",
    "        \n",
    "        print(f\"[Watcher] Detected new file: {event.src_path}\")\n",
    "        try:\n",
    "            result_html = process_audio_and_classify(event.src_path)\n",
    "            result_filename = f\"result_{uuid.uuid4().hex}.html\"\n",
    "            result_path = os.path.join(DEBUG_DIR, result_filename)\n",
    "            with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(result_html)\n",
    "            webbrowser.open(f\"file://{os.path.abspath(result_path)}\")\n",
    "            relative_path = os.path.relpath(event.src_path, RECORDER_DIR)\n",
    "            processed_path = os.path.join(PROCESSED_DIR, relative_path)\n",
    "            os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
    "            shutil.move(event.src_path, processed_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[Watcher] Error: {e}\")\n",
    "\n",
    "def start_file_watcher():\n",
    "    observer = Observer()\n",
    "    handler = NewWavHandler()\n",
    "    observer.schedule(handler, path=RECORDER_DIR, recursive=True)\n",
    "    observer.start()\n",
    "    print(\"[Watcher] Started.\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n",
    "threading.Thread(target=start_file_watcher, daemon=True).start()\n",
    "\n",
    "# ========== Gradio UI with Text and Audio ==========\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üß† Bank Call Classifier\")\n",
    "    gr.Markdown(\"Classify topics either by uploading a call recording or by entering a call transcription.\")\n",
    "\n",
    "    with gr.Tab(\"üéôÔ∏è Audio Upload\"):\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"Upload Call Audio (WAV)\")\n",
    "        status_message = gr.HTML()\n",
    "        result_output = gr.HTML()\n",
    "        run_btn = gr.Button(\"Analyze Audio\")\n",
    "\n",
    "        def show_processing_msg(audio_path):\n",
    "            return \"<b>‚è≥ Processing, please wait...</b>\", \"\"\n",
    "\n",
    "        def analyze(audio_path):\n",
    "            result = process_audio_and_classify(audio_path)\n",
    "            return \"<b>‚úÖ Done!</b>\", result\n",
    "\n",
    "        run_btn.click(fn=show_processing_msg, inputs=audio_input, outputs=[status_message, result_output]) \\\n",
    "               .then(fn=analyze, inputs=audio_input, outputs=[status_message, result_output])\n",
    "\n",
    "    with gr.Tab(\"üìù Text Input\"):\n",
    "        text_input = gr.Textbox(lines=4, placeholder=\"Paste or type the call transcription here...\", label=\"Call Transcription\")\n",
    "        text_output = gr.Text(label=\"Predicted Topic\")\n",
    "        text_btn = gr.Button(\"Classify Text\")\n",
    "\n",
    "        text_btn.click(fn=classify_with_ffnn, inputs=text_input, outputs=text_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe216f-db5b-4977-9ee2-92e3103e2333",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a448bd1-c40b-44bc-97a2-9211ef5edf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# === Load BERT tokenizer and model ===\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# === Load trained classifier and LabelEncoder ===\n",
    "classifier_model = joblib.load(\"C:/Pasha-PoC/Topic-Modeling/lrm.pkl\")\n",
    "label_encoder = joblib.load(\"C:/Pasha-PoC/Topic-Modeling/label_encoder.pkl\")\n",
    "\n",
    "# === Prediction function ===\n",
    "def classify_call(text):\n",
    "    text = text.lower().strip()\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        output = bert_model(**encoded)\n",
    "        cls_embedding = output.pooler_output.detach().numpy().squeeze()\n",
    "\n",
    "    pred_encoded = classifier_model.predict([cls_embedding])[0]\n",
    "    pred_label = label_encoder.inverse_transform([pred_encoded])[0]\n",
    "    return f\"üîç Topic: {pred_label}\"\n",
    "\n",
    "# === Gradio UI ===\n",
    "demo = gr.Interface(\n",
    "    fn=classify_call,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Enter bank call transcription...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"üìû Bank Call Classifier\",\n",
    "    description=\"Enter a call transcript to predict its topic (e.g., Card Issues, Other)\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d969a-2914-4965-98bc-9bbcbd05bdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LLM)",
   "language": "python",
   "name": "llm-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
